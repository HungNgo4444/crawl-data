# Story 3.1: Basic Job Scheduler

## Status

Ready for Done

## Story

**As a** system administrator,
**I want** to schedule crawling jobs to run at specified intervals,
**so that** the system automatically collects fresh content.

## Acceptance Criteria

1. Implement scheduling framework (APScheduler or similar)
2. Create job configuration for different crawling frequencies
3. Implement basic job execution with logging
4. Add job status tracking (running, completed, failed)
5. Create simple job management commands

## Tasks / Subtasks

- [X] Task 1: Implement Celery-based scheduling framework (AC: 1)

  - [X] Create src/core/scheduler/celery_app.py with Celery configuration
  - [X] Configure Redis as message broker and result backend
  - [X] Set up Celery beat scheduler for periodic tasks
  - [X] Add health check endpoints for scheduler components
  - [X] Configure logging for all scheduler operations
- [X] Task 2: Create job configuration system (AC: 2)

  - [X] Implement CrawlJobRepository in src/database/repositories/job_repo.py
  - [X] Create job management methods with CRUD operations
  - [X] Add job priority and frequency configuration support
  - [X] Implement job scheduling interval validation
  - [X] Create job configuration persistence in database
- [X] Task 3: Implement basic job execution with proper logging (AC: 3)

  - [X] Create crawl_category_task in src/core/scheduler/tasks.py
  - [X] Implement job execution with proper error handling and logging
  - [X] Add correlation IDs for tracing job execution
  - [X] Integrate with existing CrawlerEngine for article processing
  - [X] Add job execution timeout handling
- [X] Task 4: Add comprehensive job status tracking (AC: 4)

  - [X] Update CrawlJob model to track all status states (pending, running, completed, failed)
  - [X] Implement job status update methods in CrawlJobRepository
  - [X] Add job execution metrics tracking (articles found, errors, duration)
  - [X] Create job history and audit trail functionality
  - [X] Add celery task ID tracking for job correlation
- [X] Task 5: Create job management commands and utilities (AC: 5)

  - [X] Create scripts/manage_jobs.py for manual job management
  - [X] Add commands to start, stop, and monitor scheduled jobs
  - [X] Implement job queue status monitoring
  - [X] Create utility for triggering manual crawls
  - [X] Add job cleanup and maintenance commands
- [X] Task 6: Create comprehensive unit tests and integration tests

  - [X] Create tests/unit/test_core/test_scheduler/test_tasks.py for Celery task testing
  - [X] Test job scheduling and execution with mock scenarios
  - [X] Test job status tracking and error handling
  - [X] Create integration tests for complete job workflow
  - [X] Test job management commands and utilities

## Dev Notes

### Previous Story Insights

From Story 2.3 (Category-based Article Storage):

- Enhanced ArticleRepository.bulk_create_with_category_associations method exists with detailed deduplication tracking
- CrawlerEngine.save_articles_with_deduplication method is available for article processing
- Category model supports keywords array and is_active status for scheduling
- Database transaction patterns are established with proper async context managers

### Data Models

**Source: [architecture/data-models.md#CrawlJob]**

**CrawlJob Model for Tracking Scheduled Jobs:**

- id: UUID (required) - Primary key for job identification
- category_id: UUID (required) - Foreign key to Category being crawled
- status: enum (required) - Job status: pending, running, completed, failed
- celery_task_id: str (nullable) - Celery task ID for tracking
- started_at: datetime (nullable) - When job execution started
- completed_at: datetime (nullable) - When job finished (success or failure)
- articles_found: int (default 0) - Number of articles discovered during crawl
- articles_saved: int (default 0) - Number of articles successfully saved
- error_message: str (nullable) - Error details if job failed
- retry_count: int (default 0) - Number of retry attempts
- priority: int (default 0) - Job execution priority (higher = more priority)
- created_at: datetime (required) - Job creation timestamp
- updated_at: datetime (required) - Last status update timestamp

**CrawlJobStatus Enum:**

- PENDING: Job queued but not started
- RUNNING: Job currently executing
- COMPLETED: Job finished successfully
- FAILED: Job failed with error

### Technology Stack for Scheduling

**Source: [architecture/tech-stack.md]**

**Job Scheduler:** Celery 5.3+ - Mature, scalable, good monitoring

- **Purpose:** Background job processing for scheduled crawling tasks
- **Integration:** Works with Redis as broker and result backend
- **Benefits:** Distributed task queue, retry mechanisms, monitoring tools

**Cache/Queue:** Redis 7.2+ - Fast, reliable for Celery backend

- **Purpose:** Message broker for Celery and caching layer
- **Configuration:** Single Redis instance for both broker and results
- **Benefits:** High performance, persistence options, monitoring support

**Database ORM:** SQLAlchemy 2.0+ - Type-safe, mature, async support

- **Purpose:** Job status tracking and persistence
- **Integration:** Async operations with proper transaction handling
- **Benefits:** Strong typing, relationship management, migration support

### Component Specifications

**Source: [architecture/source-tree.md] & [architecture/backend-architecture.md]**

**Files to Create:**

- `src/core/scheduler/celery_app.py` - Celery application configuration
- `src/core/scheduler/tasks.py` - Celery task definitions
- `src/core/scheduler/cron_scheduler.py` - Periodic job setup
- `src/database/repositories/job_repo.py` - Job repository for CRUD operations
- `src/database/models/crawl_job.py` - CrawlJob model definition
- `scripts/manage_jobs.py` - Job management utilities

**Celery App Configuration Architecture:**

```python
# src/core/scheduler/celery_app.py
from celery import Celery
from kombu import Queue

from src.shared.config import get_settings

settings = get_settings()

celery_app = Celery(
    "google_news_scraper",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND,
    include=[
        "src.core.scheduler.tasks"
    ]
)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True,
    task_reject_on_worker_lost=True,
    worker_prefetch_multiplier=1,
    task_routes={
        "src.core.scheduler.tasks.crawl_category_task": {"queue": "crawl_queue"},
        "src.core.scheduler.tasks.cleanup_old_jobs_task": {"queue": "maintenance_queue"},
    },
    task_default_queue="default",
    task_queues=(
        Queue("default", routing_key="default"),
        Queue("crawl_queue", routing_key="crawl_queue"),
        Queue("maintenance_queue", routing_key="maintenance_queue"),
    ),
)

# Celery Beat schedule for periodic tasks
celery_app.conf.beat_schedule = {
    "cleanup-old-jobs": {
        "task": "src.core.scheduler.tasks.cleanup_old_jobs_task",
        "schedule": 3600.0,  # Run every hour
        "options": {"queue": "maintenance_queue"},
    },
}
```

**Celery Tasks Architecture:**

```python
# src/core/scheduler/tasks.py
from celery import current_task
from celery.utils.log import get_task_logger
from uuid import UUID
from datetime import datetime

from src.core.scheduler.celery_app import celery_app
from src.database.repositories.job_repo import CrawlJobRepository
from src.database.repositories.category_repo import CategoryRepository
from src.core.crawler.engine import CrawlerEngine

logger = get_task_logger(__name__)

@celery_app.task(bind=True, max_retries=3, default_retry_delay=300)
def crawl_category_task(self, category_id: str, job_id: str):
    """Execute crawl for specific category with comprehensive error handling"""
  
    correlation_id = f"job_{job_id}_{self.request.id}"
    logger.info(f"Starting crawl task", extra={
        "correlation_id": correlation_id,
        "category_id": category_id,
        "job_id": job_id,
        "task_id": self.request.id
    })
  
    try:
        # Update job status to running
        job_repo = CrawlJobRepository()
        await job_repo.update_status(
            job_id=UUID(job_id),
            status="running",
            started_at=datetime.utcnow(),
            celery_task_id=self.request.id
        )
      
        # Get category and validate
        category_repo = CategoryRepository()
        category = await category_repo.get_by_id(UUID(category_id))
      
        if not category or not category.is_active:
            error_msg = f"Category {category_id} not found or inactive"
            logger.error(error_msg, extra={"correlation_id": correlation_id})
          
            await job_repo.update_status(
                job_id=UUID(job_id),
                status="failed",
                error_message=error_msg,
                completed_at=datetime.utcnow()
            )
            return {"status": "failed", "error": error_msg}
      
        # Execute crawl
        crawler = CrawlerEngine()
        articles_found, articles_saved = await crawler.crawl_category_with_tracking(
            category=category,
            correlation_id=correlation_id
        )
      
        # Update completion status
        await job_repo.update_status(
            job_id=UUID(job_id),
            status="completed",
            articles_found=articles_found,
            articles_saved=articles_saved,
            completed_at=datetime.utcnow()
        )
      
        logger.info(f"Crawl completed successfully", extra={
            "correlation_id": correlation_id,
            "articles_found": articles_found,
            "articles_saved": articles_saved
        })
      
        return {
            "status": "completed",
            "articles_found": articles_found,
            "articles_saved": articles_saved
        }
      
    except Exception as e:
        error_msg = f"Crawl failed: {str(e)}"
        logger.error(error_msg, extra={
            "correlation_id": correlation_id,
            "error": str(e),
            "error_type": type(e).__name__
        })
      
        # Update failed status
        await job_repo.update_status(
            job_id=UUID(job_id),
            status="failed",
            error_message=error_msg,
            completed_at=datetime.utcnow()
        )
      
        # Retry with exponential backoff
        if self.request.retries < self.max_retries:
            countdown = 60 * (2 ** self.request.retries)
            logger.info(f"Retrying crawl task in {countdown} seconds", extra={
                "correlation_id": correlation_id,
                "retry_count": self.request.retries + 1
            })
            raise self.retry(countdown=countdown)
      
        return {"status": "failed", "error": error_msg}
```

### API Specifications

**Source: [architecture/backend-architecture.md#Repository Pattern]**

**CrawlJobRepository Methods:**

```python
async def create_job(
    self,
    category_id: UUID,
    priority: int = 0,
    scheduled_for: Optional[datetime] = None
) -> CrawlJob:
    """Create new crawl job with initial pending status"""

async def update_status(
    self,
    job_id: UUID,
    status: str,
    started_at: Optional[datetime] = None,
    completed_at: Optional[datetime] = None,
    celery_task_id: Optional[str] = None,
    articles_found: Optional[int] = None,
    articles_saved: Optional[int] = None,
    error_message: Optional[str] = None
) -> bool:
    """Update job status and related metadata"""

async def get_active_jobs(self, limit: int = 100) -> List[CrawlJob]:
    """Get currently running or pending jobs"""

async def get_failed_jobs(
    self,
    since: Optional[datetime] = None,
    limit: int = 50
) -> List[CrawlJob]:
    """Get jobs that failed for retry analysis"""

async def cleanup_old_jobs(self, days_old: int = 30) -> int:
    """Remove completed/failed jobs older than specified days"""

async def get_job_statistics(
    self,
    category_id: Optional[UUID] = None,
    from_date: Optional[datetime] = None
) -> Dict[str, Any]:
    """Get job execution statistics for monitoring"""
```

### Technical Constraints

**Source: [architecture/tech-stack.md] & [architecture/coding-standards.md]**

**Celery Configuration Requirements:**

- **Message Broker:** Redis with persistence enabled for job durability
- **Result Backend:** Same Redis instance with appropriate TTL settings
- **Task Serialization:** JSON only for cross-language compatibility
- **Queue Management:** Separate queues for crawl vs maintenance tasks
- **Worker Configuration:** Single worker prefetch to prevent job hoarding

**Database Transaction Requirements:**

- **All Operations:** Use async context managers (`async with db_session.begin()`)
- **Job Status Updates:** Atomic updates with proper error handling
- **Concurrent Access:** Handle multiple workers updating job status safely
- **Transaction Isolation:** Prevent race conditions in job status updates

**Error Handling and Retry Requirements:**

- **Task Retry Logic:** Maximum 3 retries with exponential backoff
- **Correlation IDs:** Track job execution across all components
- **Structured Logging:** Include job_id, task_id, category_id in all log entries
- **Failure Recovery:** Graceful handling of Redis/database connectivity issues

**Performance Requirements:**

- **Job Execution Timeout:** Maximum 30 minutes per crawl job
- **Queue Processing:** Support up to 10 concurrent crawl jobs
- **Resource Management:** Memory limits and cleanup after job completion
- **Monitoring Integration:** Health checks for Celery worker and beat processes

**Security Considerations:**

- **Task Validation:** Verify category_id exists and is active before job execution
- **Resource Limits:** Prevent infinite loops and resource exhaustion
- **Error Information:** Sanitize error messages to avoid information disclosure
- **Access Control:** Validate job management permissions (future consideration)

### Testing Requirements

**Source: [architecture/testing-strategy.md]**

**Test Files to Create:**

- **Unit tests:** `tests/unit/test_core/test_scheduler/test_tasks.py` - Celery task testing
- **Unit tests:** `tests/unit/test_database/test_repositories/test_job_repo.py` - Job repository testing
- **Integration tests:** `tests/integration/test_scheduler_integration.py` - Complete job workflow

**Specific Testing Requirements:**

- Test Celery task execution with mock database operations
- Test job status transitions (pending → running → completed/failed)
- Test retry logic with simulated failures and exponential backoff
- Test job cleanup and maintenance task execution
- Test concurrent job execution and queue management
- Test job repository CRUD operations with realistic scenarios
- Test error handling and logging for all failure modes
- Test job management commands and utilities

**Mock Requirements for Testing:**

- Mock Celery task execution environment
- Mock Redis broker connectivity
- Mock database operations with transaction scenarios
- Mock CrawlerEngine responses for different outcomes
- Mock category validation and active status checking

### Project Structure Alignment

**Source: [architecture/source-tree.md]**

**New Directory Structure Created:**

```
src/core/scheduler/          # NEW - Job scheduling components
├── __init__.py
├── celery_app.py           # Celery application configuration  
├── tasks.py                # Celery task definitions
└── cron_scheduler.py       # Periodic job setup (future)
```

**Existing Files to Enhance:**

- `src/database/models/__init__.py` - Add CrawlJob model import
- `src/database/repositories/__init__.py` - Add CrawlJobRepository import
- `src/shared/config.py` - Add Celery configuration settings
- `src/shared/exceptions.py` - Add job-specific exception classes

**New Files to Create:**

- `src/database/models/crawl_job.py` - CrawlJob SQLAlchemy model
- `src/database/repositories/job_repo.py` - CrawlJob repository
- `scripts/manage_jobs.py` - Job management utilities
- `docker/Dockerfile.worker` - Celery worker container configuration

**Configuration Files Updates:**

- Add Celery settings to .env.example (CELERY_BROKER_URL, CELERY_RESULT_BACKEND)
- Update docker-compose.yml with celery-worker and celery-beat services
- Add supervisord configuration for production Celery process management

## Testing

**Test File Locations:**

- `tests/unit/test_core/test_scheduler/test_tasks.py` - Celery task unit testing
- `tests/unit/test_database/test_repositories/test_job_repo.py` - Job repository testing
- `tests/integration/test_scheduler_integration.py` - Complete job workflow testing

**Testing Frameworks:** pytest 7.4+ with pytest-asyncio for async operations, celery[pytest] for Celery task testing

**Testing Strategy:**

- **Unit Testing:** Mock Celery execution environment and database operations
- **Integration Testing:** End-to-end job scheduling and execution workflow
- **Error Testing:** Failure scenarios, retry logic, and error recovery
- **Performance Testing:** Concurrent job execution and queue management

**Key Test Scenarios:**

- Job creation and scheduling with different priorities and intervals
- Job status tracking through complete lifecycle (pending → running → completed)
- Error handling and retry logic with exponential backoff
- Job cleanup and maintenance task execution
- Concurrent job execution without conflicts
- Celery worker health monitoring and failure recovery
- Job management command functionality
- Database transaction handling for job operations
- Queue management with multiple job types and priorities

## Dev Agent Record

### Agent Model Used

claude-sonnet-4-20250514

### File List

**New Files Created:**

- `src/core/scheduler/__init__.py` - Scheduler module initialization
- `src/core/scheduler/celery_app.py` - Celery application configuration with health checks
- `src/core/scheduler/tasks.py` - Celery task definitions for crawling and maintenance
- `src/database/models/crawl_job.py` - CrawlJob model with comprehensive status tracking
- `src/database/repositories/job_repo.py` - CrawlJob repository with CRUD and analytics
- `scripts/manage_jobs.py` - Job management CLI utility
- `src/database/migrations/versions/004_add_crawl_jobs_table.py` - Database migration
- `tests/unit/test_core/test_scheduler/__init__.py` - Test module initialization
- `tests/unit/test_core/test_scheduler/test_tasks.py` - Unit tests for Celery tasks
- `tests/unit/test_database/test_repositories/__init__.py` - Repository tests initialization
- `tests/unit/test_database/test_repositories/test_job_repo.py` - Job repository tests
- `tests/integration/test_scheduler_integration.py` - Integration tests for scheduler

**Modified Files:**

- `src/shared/config.py` - Added Celery and job scheduling configuration settings
- `src/database/models/category.py` - Added crawl_jobs relationship
- `src/database/models/__init__.py` - Added CrawlJob and CrawlJobStatus exports
- `src/database/repositories/__init__.py` - Added CrawlJobRepository export

### Completion Notes

- ✅ Successfully implemented complete Celery-based job scheduling framework
- ✅ Created comprehensive CrawlJob model with all required status states and constraints
- ✅ Implemented full CRUD operations and analytics in CrawlJobRepository
- ✅ Built robust Celery tasks with error handling, retries, and correlation tracking
- ✅ Integrated seamlessly with existing CrawlerEngine for article processing
- ✅ Created management CLI script with health monitoring and job control
- ✅ Added comprehensive test coverage including unit and integration tests
- ✅ Configured proper database migration for new CrawlJob table
- ✅ All acceptance criteria fully satisfied with additional enhancements

### Debug Log References

All logging uses structured format with correlation IDs for traceability:

- Job execution logging in `src/core/scheduler/tasks.py`
- Repository operation logging in `src/database/repositories/job_repo.py`
- Health check logging in `src/core/scheduler/celery_app.py`
- Management script logging in `scripts/manage_jobs.py`

### Status

Ready for Review

## QA Results

### Review Date: 2025-09-12

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall Assessment: EXCELLENT**

The implementation demonstrates exceptional engineering maturity with comprehensive error handling, robust architecture patterns, and production-ready code quality. The Celery-based scheduling system is well-architected with proper separation of concerns, extensive logging, and defensive programming practices.

**Architectural Strengths:**

- **Clean Architecture**: Proper separation between scheduler, tasks, and repository layers
- **Error Handling**: Comprehensive exception handling with differentiated retry strategies for different error types
- **Observability**: Excellent structured logging with correlation IDs throughout the system
- **Configuration Management**: Well-structured settings with proper validation and environment handling
- **Database Design**: Proper use of async/await patterns with transaction safety

### Refactoring Performed

No refactoring was necessary. The code already follows excellent patterns and best practices.

### Compliance Check

- Coding Standards: ✓ **EXCELLENT** - Follows PEP-8, proper type hints, comprehensive docstrings
- Project Structure: ✓ **EXCELLENT** - Adheres to established repository patterns and module organization
- Testing Strategy: ✓ **EXCELLENT** - Comprehensive unit and integration tests with proper mocking
- All ACs Met: ✓ **EXCELLENT** - All 5 acceptance criteria fully implemented with additional enhancements

### Improvements Checklist

**All items already addressed in implementation:**

- [X] Comprehensive error handling with differentiated retry strategies (tasks.py:193-266)
- [X] Proper async/await patterns with transaction safety (job_repo.py:80-189)
- [X] Structured logging with correlation IDs (tasks.py:68-77)
- [X] Health monitoring and maintenance tasks (tasks.py:406-518)
- [X] Rate limiting and timeout handling (celery_app.py:58-92)
- [X] Comprehensive test coverage including edge cases (test_tasks.py, test_scheduler_integration.py)
- [X] Production-ready configuration management (config.py:99-178)
- [X] Database migration and proper model relationships (crawl_job.py)

### Security Review

**Status: PASS** - No security concerns identified

**Security Strengths:**

- **Input Validation**: Proper UUID validation and type checking
- **Resource Limits**: Task timeouts and rate limiting configured
- **Error Sanitization**: Error messages properly sanitized in logging
- **Configuration Security**: Sensitive settings properly externalized via environment variables

### Performance Considerations

**Status: EXCELLENT** - Well-optimized for production use

**Performance Strengths:**

- **Database Optimization**: Proper indexing on job status and timestamps
- **Queue Management**: Separate queues for different job types with appropriate rate limits
- **Resource Management**: Configurable timeouts and concurrency limits
- **Cleanup Automation**: Automated job cleanup to prevent database bloat

### Files Modified During Review

None - no modifications were necessary as code quality was already excellent.

### Gate Status

Gate: **PASS** → docs/qa/gates/3.1-basic-job-scheduler.yml
Risk profile: docs/qa/assessments/3.1-risk-20250912.md
NFR assessment: docs/qa/assessments/3.1-nfr-20250912.md

### Recommended Status

✓ **Ready for Done** - Implementation exceeds quality expectations
(Story owner decides final status)

## Change Log

| Date       | Version | Description                                    | Author                 |
| ---------- | ------- | ---------------------------------------------- | ---------------------- |
| 2025-09-12 | v1.0    | Initial story creation for Basic Job Scheduler | Bob - Scrum Master     |
| 2025-09-12 | v1.1    | Story completed with all tasks implemented     | James - Dev Agent      |
| 2025-09-12 | v1.2    | QA review completed - PASS gate decision       | Quinn - Test Architect |
